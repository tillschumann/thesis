% Ein Beispielkapitel
%

\chapter{Implementation}

The implementation which are done in this thesis aim at enabling the building and simulation
of a full mouse brain model relaying on point neuron model. Such a model must be build by
reconstructing parts of the model from experimental data provided by both the Allen Institute for
Brain for Science and the Blue Brain Project
To this aim, an efficient data format is defined, the circuit generation scripts are adapted to
run on a bigger scale and the NEST simulator is extended to load the new data format.

\section{Data formats}
A point neuron model contains point neurons and connections (synapses) between the point neurons.
The same point neuron model (conductance based exponential integrate-and-fire neuron model \cite{brette2005adaptive}) and synapse model (Tsodyks synapse model containing synaptic short-term depression and short-term facilitation \cite{tsodyks1997neural, fuhrmann2002coding}) are used for all neurons and synapses respectively.
The neurons and synapses are characterized by parameters.
Each neuron is defined by a given set of parameters.
The synapses contain besides the parameters the source and target neuron id, which the synapses connects.
To store the circuit in files two different data formats are defined.
The first data format defines the storage for all neuron parameters.
The second data format defines the storage for the synapse parameters and source and target neuron ids.

The data format for the neurons and the synapses follow two different concepts.
The neuron data format contains multiple datasets containing the neuron parameters.
All datasets have by definition the same length. This length defines the number of neurons.
The neuron parameters of neuron $i$ are stored in the i-th line of each data set.
Besides the neuron parameters the file contains attributes for storing meta data of the circuit.
\begin{figure}[ht!]
   	\begin{center}
        \subfigure[HDF5 file format for the neurons]{%
            \label{fig:allInjections}
            \includegraphics[scale=0.5]{pictures/hdf5_neuron_format.eps}
        }
        \hspace{1cm}
        \subfigure[HDF5 file format for the synapses]{%
            \label{fig:oneProjection}
            \includegraphics[scale=0.41]{pictures/hdf5_syn_format.eps}
       }
    	   \end{center}
    	\caption{%
        You can see the data format with their related datasets and data types. 
        How they are created by the circuit generation and loaded by the NEST import module.
     }%
   \label{fig:atlas}
   \end{figure}
The synapse data format contains only two datasets. In general the datasets contain for each synapse
a source neuron id, a target neuron id and a set of parameters. To reduce the amount of data.
The source neuron ids are grouped together in the \emph{neuron} dataset. This is feasible, because
the number of synapses is way bigger than the number of source neurons ($~11,000$ synapses per source neuron).
The \emph{syn\_ptr} value defines the related starting index in the \emph{syn} dataset,
respectively \emph{syn\_n} defines the number of related following entries.
The \emph{syn} dataset contains the target neuron id and a set of parameters.
Both datasets use a compound data type to store different data types in the same dataset.

\newpage
\section{Circuit generation}
Rewriting the sequential python script to a hybrid C++ application require a parallel implementation of the algorithm and the usage of 
a parallel random generator. For the parallelization strategy a Master-Slave approach is chosen to distribute the workload dynamically on the nodes.
Communication between the individual nodes is not necessary. Only the workload management is handled by the master node.
For the random generator Random123\cite{salmon2011parallel} is used.
It ensures reproducible results without correlations in the generated values, which is essential.


%%insert somewhere
%Unfortunately all injections from these experiments do not
%cover the whole brain. So there are neurons which are not injected
%by any experiment. Therefore all neurons which are not injected should use the projection
%from the nearest injection.


\subsection{Long range connections}

The long range generation algorithm is adapted to be parallelizable efficiently.
The problem with the sequential algorithm is, that it iterates through all experiments
and generates all connections for all the voxels inside its injection regions, if 
all experiments before have not touched the voxels or have higher total injection
value. This means it generates connections which might be overwritten and each iteration might depend
on all previous iterations. To overcome the dependency the best chosen injection per voxel is calculated before hand.
The number of written entries can be calculated before hand. This allows to create
the HDF5 dataset before and assign a writing position to each voxel.
Thus all nodes can write independent to the file system.
After that all voxels are distributed to the nodes and inside the iteration over all neurons
inside the given voxel, the sequential algorithm can be applied on each node.
An efficient parallel implementation depends on the distribution of the voxels, which are
work packages for each node. The work load has to be distributed equally on all nodes.
Therefore the determination of the wall-clock time needed for each voxel is necessary.
But it \ref{eq:L} is not constant.
So the algorithm can be partitioned into five parts: linear rejection (\emph{linear}), storing to disk \emph{store}(),
loading experiment data (\emph{load}), select neurons in injection (\emph{selI}), select neurons in projection (\emph{selP}).
\begin{equation} \label{eq:L}
	L \approx L_{linear}(n,m,M) + L_{store}(n,\omega) + L_{load}(\nu) + L_{selI}(n) + L_{selP}(m,\nu)
\end{equation}
$L_{load}(\nu)$, $L_{selI}(n)$ and $L_{selP}(m,\nu)$ are neglectable.
$\Omega$ and $\frac{\partial L_{linear}(n,m,M)}{\partial M}$  can not be estimated satisfyingly.
$\Omega$ represents waiting times, which occur from independent parallel write operations.
$M$ represents the distribution of probability values.
The distribution effects the number rejections inside the linear rejection method.
Therefore it has a significant impact on the wall-clock time.
Thus there is only a rough estimation of the wall-clock time per voxel iteration possible.
\begin{equation} \label{eq:Lapprox}
	L_{estimate} = L_{linear}(n,m) + L_{store}(n)
\end{equation}
Therefore a statically and dynamically voxel distribution is implemented.
At first a third of all voxels are distributed based on $L_{estimate}$ \ref{eq:Lapprox}.
After that the master node distributes further voxels in sets on request from the other nodes.
Figure \ref{fig:longrangParallel} illustrates the workflow of all nodes.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{pictures/longRange_parallelAlg.eps}
\caption{Task distribution between the nodes for the long range connections generation. It illustrates which tasks are distributed between which nodes.}
\label{fig:longrangParallel}
\end{figure}
\emph{Node 0} is selected as the master node. It does not participate on the iterations.
It manages the dynamic distribution of the voxels to the other nodes and 
assigns write positions to each voxels. Thus each node receives besides the 
voxel information the positions,
where it has to write its data to the HDF5 datasets.
\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{pictures/longRange_BestExp_parallelAlg.eps}
\caption{Subtasks of the above listed \emph{Load experiment per voxel} task}
\label{fig:longrangeLEPV}
\end{figure}
Task \emph{Load experiment per voxel} from Figure \ref{fig:longrangParallel} contain several subtasks.
It generates the voxel datasets, which are distributed afterwards.
Manipulations of the generation effect mostly this part.
So the part allows modifications of the applied functionalities. 
Further the whole function can be skipped by loading the voxel information from file \ref{file:voxelinfo}.
Figure \ref{fig:longrangeLEPV} shows the subtasks.
The red tasks are optional and can be activated via command line arguments.
In the first task \emph{Find best experiment per voxel} each node loads a set of experiments.
Iteratively the experiment with the smallest total injection density is assigned to each voxel
on each node. Afterwards a MPI reduction function returns the smallest value for each voxel on
each node.
But there are regions which were not injected by any experiment available.
So there is no connection information available for the internal neurons.
To overcome the missing data a piecewise constant interpolation is applied to the chosen experiment per voxel
in \emph{Interpolate values for empty voxels}.
It is realised with an distributed iteratively nearest neighbour search algorithm for each missing voxel.
Figure \ref{longrange} shows an illustration of the search algorithm
projected in a 2D plane. 
\begin{figure}[ht!]
\centering
\includegraphics[scale=2.5]{pictures/longRange_Nearest_parallelAlg.eps}
\caption{Find nearest neighbor loop}
\label{longrange}
\end{figure}
The first voxel along the search direction which is found is declared as the nearest neighbour.
The chosen experiment from the nearest neighbour is adopted.

Afterwards the voxels are mirrored from the right to the left hemisphere, because the
Allen Brain Atlas mostly delivers injection sites for the right hemisphere and
the mouse brain model is assumed to be symmetric.

Finally all voxel information are gathered to all nodes.
For debugging or manipulating purposes function \emph{Save voxel properties to HDF5 file} can be used
to save the generated voxel information to file \ref{file:voxelinfo}.

%\begin{figure}[ht!]
%   	\begin{center}
%        \subfigure[All nodes are equal and process the work based on a static distribution]{%
%            \label{fig:shortColumn}
%            \includegraphics[scale=0.5]{pictures/All_Worker_Collective.eps}
%        }
%        \hspace{1cm}
%        \subfigure[The 0 node is assigned as the master node and handles from this on the management of the work distribution.]{%
%            \label{fig:shortColumnInCircuit}
%            \includegraphics[scale=0.5]{pictures/MasterWorker.eps}
%       }
%    	   \end{center}
%    	\caption{%
%        The figure illustrates the work distribution and communication between the nodes.
%     }%
%   \label{fig:atlas}
%   \end{figure}


\subsection{Short range connections}

The algorithm presented in the analysis section can be parallelized straight forward.
Each iteration is independent. The challenging part is the distribution of the iterations and
the writing to disk. The number ob synapses which are created is not know. It is only available
at the end.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{pictures/shortRange_parallelAlg.eps}
\end{figure}

The short range algorithm is parallelized using a master-worker strategy to achieve a good load balancing.
The master node manages the distribution of the neurons, which can be seen as task units. For each neuron 
is synapses have to be created. The workers load in the first step all needed matrices from the file system.
Then each node request a set of neurons from the master node. Over this set it iterates and creates resulting
synapses. For \emph{Generate P} Random123 as a random number generator is used. Therefore a parallel usage of 
the given sequential algorithm is possible. Only the storage of the generated synapse list, has to be done 
after all iterations. So in each loop the synapse list is stored in a buffer.
If all neurons are distributed and processed all nodes, including the master node, calculate the position in 
the HDF5 dataset where each node is allowed to write. After that all workers write theirs synapse lists to
the HDF5 dataset.


\begin{figure}[ht!]
   	\begin{center}
        \subfigure[Master-worker strategy to get good balance properties]{%
            \label{fig:shortColumn}
            \includegraphics[scale=0.5]{pictures/MasterWorker.eps}
        }
        \hspace{1cm}
        \subfigure[Collective write operation to maximize used bandwidth]{%
            \label{fig:shortColumnInCircuit}
            \includegraphics[scale=0.5]{pictures/Worker_Collective.eps}
       }
    	   \end{center}
    	\caption{%
        The figure illustrates the work distribution and communication between the nodes.
     }%
   \label{fig:atlas}
   \end{figure}
   
\begin{itemize}
      \item memory and computation requirements
\end{itemize}


\newpage
\section{Circuit validation}

In the context of this thesis the circuit is validated in terms of geometrical correct placement of
the synapses. So the position of the source and target can be visualized and be compared to a 
reference system. For the long and the short range connectivity the geometrical shape is know.
In particular the long range connectivity should connect neurons from the injected regions to neurons
in the projected region for each used experiment. Therefore a circuit for a single experiment is 
generated, the target and source neurons are visualized in a 3D coordinate system and the the injection
and projection is used as the reference system. If source neurons are inside the injection space and all
target neurons are in the projection space, the implementation is correct.
For the short range connectivity all post-synaptic neurons of synapses having the same pre-synaptic neuron should
lay inside a cylinder in layer 1 to 6. Visualising the post-synaptic neurons geometrical on top of the contour of layer 1 to 6 allows to review if the shape matches with a cylinder.
To perform the visual validation the renderer voxalize \ref{voxelize} is used with the visualization tool Paraview \ref{paraview}.
 \begin{figure}[ht!]
\centering
\includegraphics[width=0.4\textwidth]{pictures/paraview_ex.png}
\end{figure}
For the validation the BBP visualization team offered a rendering pipeline to visualize the placement of target neurons for each synapse file. Therefore the neuron and synapse HDF5 can be imported into 
Voxelize \ref{voxelize}. Voxelize creates a voxelized dataset out of the target neuron positions, which can be exported into
a MetaImage \ref{MetaImage} file or directly viewed with Livre \ref{Livre}.
The MetaImage files can be viewed with Paraview \ref{Paraview}.
Injection and projection images are also given in the MetaImage format.
A direct comparison inside of Paraview is possible.
Loading the projection inside of Paraview and applying the contour filter on it with a threshold of $0.01$ (threshold is also applied to the same data in the long range connectivity generation) allows to visualize the valid boundaries for all target neurons for this particular experiment.
 
\begin{itemize}
      \item Pictures of both validation
      \item Visual validation of spiking activity
      \item Statistical properties are not analyzed in this work 
\end{itemize}

\section{NEST import modules}
The neuronal spiking network simulator NEST is developed in \emph{C++} and delivers
an user interface based on an own description language \emph{SLI} and  and a Python interface.
The new use case shall be integrated into the standard work flow of NEST.
Besides the functionality in \emph{C++} the interfaces have to be extended.
The difficulties of the network generation is based on a difference in 
the NEST internal data structure and the data delivered by the Allen Institute.
Connection information contains target and source neurons besides biochemical
information of the synapses. Because of the in vitro injection methods the
connection information maps the synapse from the source to the target neurons.
For multi process simulations NEST distributes all neurons based on a modulo function 
to the processes. Because of memory optimizations the synapses are only stored on the
post synaptic process. This means that the connection information is stored
on the process, where the target neuron is located. Therefore a transformation of the given data is
necessary. Preprocessing of the input data should be avoided as far as possible to capture
future use cases.
The resulting implementation shall load the connection information efficiently in parallel,
distribute the synapse information to the post synaptic node and store it in
the NEST data structure.
Further requirements of the implementation are an efficient use of the available resources as
memory and computation power. 


\subsubsection{Import neurons}
To import neurons into NEST using the internal C++ API NEST has to create the requested number of neurons and
assign their parameters afterwards. In an distributed environment the create function has to be called on all nodes. But assigning the parameters has only be done on the nodes, where the neurons are placed on.
Thus only a set of the parameter datasets is needed on each node.
To group neurons together to subnets, neurons can be created inside of virtual subnets.
This subnets have to be created before hand.
\begin{algorithm}
 \KwData{HDF5 neuron dataset}
 \KwResult{Created neurons inside NEST data structure}
 Find unique values in subnet dataset; \\
 Create subnet for each unique entry; \\
 Create neurons based on length of HDF5 datasets with given neuron type inside specified subnets; \\
 Read parameter datasets collectively; \\
 Assign parameters to neurons;
\label{alg2}
\caption{Implemented import neurons algorithm}
\end{algorithm}
Therefore the implemented algorithm sorts out the needed subnets in its first step.
Unique values are extracted from the subnet dataset.
After that for each unique value a subnet is created.
In the next step the neurons are iteratively created inside the specified subnet.
Then the parameter values are loaded from the HDF5 datasets and committed to the neurons.
The neurons are stored distributed on the nodes.
So the neuron parameters are only needed on the node, where the neuron is placed.
The distribution is known. NEST distributes the neurons based on the modulo of their ids. Neuron $i$ is placed on node $i mod number\_of\_nodes$. Each node has to load all needed parameter entries
from the datasets. Derived from the distribution function each node has to load each number\_of\_nodesths entry starting from its node id.

\newpage
\subsubsection{Import synapses}
Synapse import module reads the the synapse dataset block wise. Using hyperslabs each node selects a block from the
dataset and copy it to memory with all the specified parameters.
The source neuron list and the target neurons and their parameters from each block are used to create a list of all synapses.
For each entry the target node is determined.
The list is sorted by the target nodes.
Using \emph{MPI\_Alltoall} the list can be distributed directly to the corresponding nodes.
Afterwards each node contains its list of synapses and can use the \emph{NEST} connect function, to copy it to its internal data structure.
\begin{algorithm}
	\KwData{List of HDF5 files for each node, chunk size}
	\KwResult{Connected NEST network}
	\While{Chunk to read in HDF5 files}{
		Read block and store in memory \hspace{45px}(1)\;
 Determine target node for each synapse \hspace{63px}(1,2) \hspace{36px}$\mathcal{O}(n)$\;
 Sort synapses by target nodes \hspace{66px}(1,2) \hspace{28px}$\mathcal{O}(n^2)$\;
 MPI\_Alltoallv using sorted list \hspace{76px}(1,2,3,4)\;
 Connect all synapses using NEST function \hspace{36px}(1,2) \hspace{45px}$\mathcal{O}(n)$\;
	}

\caption{Implemented import synapses algorithm, $S_i$ source neuron $i$, $Tn_i$ target neuron $i$.
	set in brackets contains current needed variables}
\label{alg2}
\end{algorithm}



\begin{figure}[ht!]
\centering
\includegraphics[scale=0.4]{pictures/Connect_inside_iteration.eps}
\caption{Inside iteration parallel workflow}
\label{Algparts}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.7]{pictures/sort_table_all_alltoall.eps}
\caption{Distribute synapses to nodes.}
\end{figure}


\lstdefinestyle{cppcode} {language=C++,
                basicstyle=\tiny\ttfamily,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{green}\ttfamily,
                morecomment=[l][\color{magenta}]{\#}
                numbers=left,
  				stepnumber=5,    
  				firstnumber=1,
 				numberfirstline=true
}
\begin{figure}[ht!]
\begin{lstlisting}[style=cppcode]
    void H5Synapses::singleConnect(NESTNodeSynapse& synapse,
    	nest::index synmodel_id,
    	nest::Node* const target_node,
    	const nest::thread target_thread,
    	uint64_t& n_conSynapses)
{
  nest::index source = synapse.source_neuron_;
  
  // check whether the target is on this process
  if (nest::NestModule::get_network().is_local_node(target_node)) {
    // use region to allow setting lock for creating/destroying token
    {     
      DictionaryDatum d( new Dictionary );
      
      //new/delete Token is not thread-safe
      omp_set_lock(&tokenLock);
      for (int i=0; i<synapses_.prop_names.size(); i++)
	def< double_t >( d, synapses_.prop_names[i], synapses_.prop_facts[i] * synapse.prop_values_ [i] + param_offset[i]);
      omp_unset_lock(&tokenLock);

      bool success = nest::NestModule::get_network().connect(source, target_node->get_gid(), d, synmodel_id);
 
      if (success)
	n_conSynapses++;
      
      omp_set_lock(&tokenLock);
    }
    omp_unset_lock(&tokenLock);
  }
  else
  {
    throw nest::IllegalConnection("H5Synapses::singleConnect(): synapse is on wrong node");
  }
}
\end{lstlisting}
\caption{Code copied from the import module, showing the used NEST c++ api.}
\end{figure}

\begin{itemize}
      \item describe use of NEST c++ api
\end{itemize}

\newpage
\section{Memory consumption}
\begin{figure}[ht!]
\begin{tabular}{| l | l | l | l |}
    \hline
    (id) & data structures & memory consumption \\ \hline
    (1) & block in memory & $sizeof(float)*h*p$ \\ \hline
    (2) & target neuron node list & $sizeof(int)*h$ \\ \hline
    (3) & MPI send vectors & $(sizeof(float)*p + sizeof(int)) * h$ \\ \hline
    (4) & MPI recv vectors & $(sizeof(float)*p + sizeof(int)) * \frac{h}{\nu(N)}$ \\ \hline
    \end{tabular}
\caption{$N$: number of nodes; $h$: size of block; $p$: number of parameters; $\nu(N)$: distribution coefficient of data}
\end{figure}
The maximum memory consumption is:
\begin{equation}
  M = (sizeof(float)*p + sizeof(int)) * 2h * (1 + \frac{1}{\nu(N)})
  \label{eq:maxmemoryconsumption}
\end{equation}
The block size $p$ is set to $1e6$ for the run on the BG/Q.
It affects the memory consumption and performance loading data from disk.
The implementation could handle different values per node.
But a block size adoption based on memory bottlenecks, is not implemented (see discussion \ref{pAdaption}).